{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Pandas and NumPy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mobile_number', 'arpu_6', 'arpu_7', 'arpu_8', 'onnet_mou_6',\n",
       "       'onnet_mou_7', 'onnet_mou_8', 'offnet_mou_6', 'offnet_mou_7',\n",
       "       'offnet_mou_8',\n",
       "       ...\n",
       "       'fb_7_2.0', 'fb_8_1.0', 'fb_8_2.0', 'total_data_amt_6',\n",
       "       'total_data_amt_7', 'total_data_amt_8', 'churn',\n",
       "       'days_forrech_before_month_end_6', 'days_forrech_before_month_end_7',\n",
       "       'days_forrech_before_month_end_8'],\n",
       "      dtype='object', length=144)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_data = pd.read_csv(\"churn_data_cleaned.csv\")\n",
    "churn_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arpu_6</th>\n",
       "      <th>arpu_7</th>\n",
       "      <th>arpu_8</th>\n",
       "      <th>onnet_mou_6</th>\n",
       "      <th>onnet_mou_7</th>\n",
       "      <th>onnet_mou_8</th>\n",
       "      <th>offnet_mou_6</th>\n",
       "      <th>offnet_mou_7</th>\n",
       "      <th>offnet_mou_8</th>\n",
       "      <th>roam_ic_mou_6</th>\n",
       "      <th>...</th>\n",
       "      <th>fb_7_1.0</th>\n",
       "      <th>fb_7_2.0</th>\n",
       "      <th>fb_8_1.0</th>\n",
       "      <th>fb_8_2.0</th>\n",
       "      <th>total_data_amt_6</th>\n",
       "      <th>total_data_amt_7</th>\n",
       "      <th>total_data_amt_8</th>\n",
       "      <th>days_forrech_before_month_end_6</th>\n",
       "      <th>days_forrech_before_month_end_7</th>\n",
       "      <th>days_forrech_before_month_end_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1069.180</td>\n",
       "      <td>1349.850</td>\n",
       "      <td>3171.480</td>\n",
       "      <td>57.84</td>\n",
       "      <td>54.68</td>\n",
       "      <td>52.29</td>\n",
       "      <td>453.43</td>\n",
       "      <td>567.16</td>\n",
       "      <td>325.91</td>\n",
       "      <td>16.23</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>378.721</td>\n",
       "      <td>492.223</td>\n",
       "      <td>137.362</td>\n",
       "      <td>413.69</td>\n",
       "      <td>351.03</td>\n",
       "      <td>35.08</td>\n",
       "      <td>94.66</td>\n",
       "      <td>80.63</td>\n",
       "      <td>136.48</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>354.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>514.453</td>\n",
       "      <td>597.753</td>\n",
       "      <td>637.760</td>\n",
       "      <td>102.41</td>\n",
       "      <td>132.11</td>\n",
       "      <td>85.14</td>\n",
       "      <td>757.93</td>\n",
       "      <td>896.68</td>\n",
       "      <td>983.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74.350</td>\n",
       "      <td>193.897</td>\n",
       "      <td>366.966</td>\n",
       "      <td>48.96</td>\n",
       "      <td>50.66</td>\n",
       "      <td>33.58</td>\n",
       "      <td>85.41</td>\n",
       "      <td>89.36</td>\n",
       "      <td>205.89</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>977.020</td>\n",
       "      <td>2362.833</td>\n",
       "      <td>409.230</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5285.0</td>\n",
       "      <td>20424.0</td>\n",
       "      <td>455.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 142 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     arpu_6    arpu_7    arpu_8  onnet_mou_6  onnet_mou_7  onnet_mou_8  \\\n",
       "0  1069.180  1349.850  3171.480        57.84        54.68        52.29   \n",
       "1   378.721   492.223   137.362       413.69       351.03        35.08   \n",
       "2   514.453   597.753   637.760       102.41       132.11        85.14   \n",
       "3    74.350   193.897   366.966        48.96        50.66        33.58   \n",
       "4   977.020  2362.833   409.230         0.00         0.00         0.00   \n",
       "\n",
       "   offnet_mou_6  offnet_mou_7  offnet_mou_8  roam_ic_mou_6  \\\n",
       "0        453.43        567.16        325.91          16.23   \n",
       "1         94.66         80.63        136.48           0.00   \n",
       "2        757.93        896.68        983.39           0.00   \n",
       "3         85.41         89.36        205.89           0.00   \n",
       "4          0.00          0.00          0.00           0.00   \n",
       "\n",
       "                ...                 fb_7_1.0  fb_7_2.0  fb_8_1.0  fb_8_2.0  \\\n",
       "0               ...                        0         1         0         1   \n",
       "1               ...                        1         0         1         0   \n",
       "2               ...                        0         1         0         1   \n",
       "3               ...                        1         0         1         0   \n",
       "4               ...                        1         0         1         0   \n",
       "\n",
       "   total_data_amt_6  total_data_amt_7  total_data_amt_8  \\\n",
       "0               0.0               0.0               0.0   \n",
       "1               0.0             354.0             207.0   \n",
       "2               0.0               0.0               0.0   \n",
       "3               0.0             712.0             540.0   \n",
       "4            5285.0           20424.0             455.0   \n",
       "\n",
       "   days_forrech_before_month_end_6  days_forrech_before_month_end_7  \\\n",
       "0                              3.0                             34.0   \n",
       "1                              5.0                             36.0   \n",
       "2                              0.0                             31.0   \n",
       "3                             12.0                             43.0   \n",
       "4                              0.0                             31.0   \n",
       "\n",
       "   days_forrech_before_month_end_8  \n",
       "0                             65.0  \n",
       "1                             67.0  \n",
       "2                             62.0  \n",
       "3                             74.0  \n",
       "4                             62.0  \n",
       "\n",
       "[5 rows x 142 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Putting feature variable to X\n",
    "X = churn_data.drop(['churn','mobile_number'], axis=1)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: churn, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Putting response variable to y\n",
    "y = churn_data['churn']\n",
    "\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, train_size=0.7, test_size=0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "smt = SMOTETomek(ratio='auto')\n",
    "X_train_balance, y_train_balance = smt.fit_sample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.599214145383105"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Checking the Churn Rate\n",
    "churn = (sum(churn_data['churn'])/len(churn_data['churn'].index))*100\n",
    "churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>y</td>        <th>  No. Observations:  </th>  <td> 37664</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>GLM</td>       <th>  Df Residuals:      </th>  <td> 37526</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>       <td>Binomial</td>     <th>  Df Model:          </th>  <td>   137</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>        <td>logit</td>      <th>  Scale:             </th> <td>  1.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -15356.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>           <td>Mon, 13 May 2019</td> <th>  Deviance:          </th> <td>  30712.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>               <td>20:12:46</td>     <th>  Pearson chi2:      </th> <td>8.91e+04</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>        <td>100</td>       <th>  Covariance Type:   </th> <td>nonrobust</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>   -1.6808</td> <td>    0.026</td> <td>  -64.105</td> <td> 0.000</td> <td>   -1.732</td> <td>   -1.629</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>   -0.1817</td> <td>    0.066</td> <td>   -2.747</td> <td> 0.006</td> <td>   -0.311</td> <td>   -0.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.4209</td> <td>    0.079</td> <td>    5.354</td> <td> 0.000</td> <td>    0.267</td> <td>    0.575</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    1.2147</td> <td>    0.094</td> <td>   12.881</td> <td> 0.000</td> <td>    1.030</td> <td>    1.400</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   -6.1407</td> <td>    1.590</td> <td>   -3.863</td> <td> 0.000</td> <td>   -9.256</td> <td>   -3.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>    1.9294</td> <td>    1.289</td> <td>    1.497</td> <td> 0.134</td> <td>   -0.597</td> <td>    4.456</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>    1.8155</td> <td>    1.465</td> <td>    1.239</td> <td> 0.215</td> <td>   -1.056</td> <td>    4.688</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>   -4.9840</td> <td>    1.641</td> <td>   -3.038</td> <td> 0.002</td> <td>   -8.200</td> <td>   -1.768</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>    0.2462</td> <td>    1.289</td> <td>    0.191</td> <td> 0.849</td> <td>   -2.280</td> <td>    2.772</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>    1.2653</td> <td>    1.505</td> <td>    0.841</td> <td> 0.400</td> <td>   -1.684</td> <td>    4.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>    0.0653</td> <td>    0.023</td> <td>    2.879</td> <td> 0.004</td> <td>    0.021</td> <td>    0.110</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>    0.0452</td> <td>    0.025</td> <td>    1.774</td> <td> 0.076</td> <td>   -0.005</td> <td>    0.095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td>   -0.0086</td> <td>    0.016</td> <td>   -0.524</td> <td> 0.600</td> <td>   -0.041</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td>    1.3590</td> <td>    0.401</td> <td>    3.391</td> <td> 0.001</td> <td>    0.573</td> <td>    2.145</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td>   -0.0396</td> <td>    0.255</td> <td>   -0.155</td> <td> 0.877</td> <td>   -0.539</td> <td>    0.460</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td>   -0.2686</td> <td>    0.334</td> <td>   -0.803</td> <td> 0.422</td> <td>   -0.924</td> <td>    0.387</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>   <td> 2127.0491</td> <td>  675.251</td> <td>    3.150</td> <td> 0.002</td> <td>  803.581</td> <td> 3450.517</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>   <td>   62.3075</td> <td>  707.191</td> <td>    0.088</td> <td> 0.930</td> <td>-1323.761</td> <td> 1448.376</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>   <td> 2968.3592</td> <td>  685.743</td> <td>    4.329</td> <td> 0.000</td> <td> 1624.327</td> <td> 4312.392</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>   <td> 2300.7460</td> <td>  730.718</td> <td>    3.149</td> <td> 0.002</td> <td>  868.564</td> <td> 3732.928</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>   <td>   62.8421</td> <td>  704.481</td> <td>    0.089</td> <td> 0.929</td> <td>-1317.915</td> <td> 1443.600</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>   <td> 3026.1187</td> <td>  698.975</td> <td>    4.329</td> <td> 0.000</td> <td> 1656.153</td> <td> 4396.085</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>   <td>  206.9519</td> <td>   65.653</td> <td>    3.152</td> <td> 0.002</td> <td>   78.275</td> <td>  335.629</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>   <td>    5.2925</td> <td>   64.438</td> <td>    0.082</td> <td> 0.935</td> <td> -121.004</td> <td>  131.589</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>   <td>  254.5721</td> <td>   58.788</td> <td>    4.330</td> <td> 0.000</td> <td>  139.350</td> <td>  369.794</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>   <td>   -0.1765</td> <td>    0.025</td> <td>   -7.065</td> <td> 0.000</td> <td>   -0.225</td> <td>   -0.128</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>   <td>   -0.0751</td> <td>    0.032</td> <td>   -2.315</td> <td> 0.021</td> <td>   -0.139</td> <td>   -0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>   <td>    0.0817</td> <td>    0.027</td> <td>    2.996</td> <td> 0.003</td> <td>    0.028</td> <td>    0.135</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th>   <td>-9540.7094</td> <td> 1361.105</td> <td>   -7.010</td> <td> 0.000</td> <td>-1.22e+04</td> <td>-6872.992</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th>   <td>-2620.4693</td> <td> 1438.547</td> <td>   -1.822</td> <td> 0.069</td> <td>-5439.970</td> <td>  199.031</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x30</th>   <td>-1547.4897</td> <td> 1429.622</td> <td>   -1.082</td> <td> 0.279</td> <td>-4349.498</td> <td> 1254.518</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x31</th>   <td> -250.4705</td> <td> 1416.055</td> <td>   -0.177</td> <td> 0.860</td> <td>-3025.887</td> <td> 2524.946</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x32</th>   <td> 2298.8751</td> <td> 1525.117</td> <td>    1.507</td> <td> 0.132</td> <td> -690.299</td> <td> 5288.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33</th>   <td> -453.4524</td> <td> 1548.518</td> <td>   -0.293</td> <td> 0.770</td> <td>-3488.492</td> <td> 2581.587</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x34</th>   <td> -254.3615</td> <td> 1430.844</td> <td>   -0.178</td> <td> 0.859</td> <td>-3058.763</td> <td> 2550.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x35</th>   <td> 2386.4856</td> <td> 1582.128</td> <td>    1.508</td> <td> 0.131</td> <td> -714.428</td> <td> 5487.399</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x36</th>   <td> -466.6791</td> <td> 1594.913</td> <td>   -0.293</td> <td> 0.770</td> <td>-3592.652</td> <td> 2659.294</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x37</th>   <td>   -7.5623</td> <td>   42.401</td> <td>   -0.178</td> <td> 0.858</td> <td>  -90.668</td> <td>   75.543</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x38</th>   <td>   72.0182</td> <td>   47.743</td> <td>    1.508</td> <td> 0.131</td> <td>  -21.556</td> <td>  165.592</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x39</th>   <td>  -12.6715</td> <td>   43.066</td> <td>   -0.294</td> <td> 0.769</td> <td>  -97.080</td> <td>   71.737</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x40</th>   <td>-9226.9267</td> <td> 2490.562</td> <td>   -3.705</td> <td> 0.000</td> <td>-1.41e+04</td> <td>-4345.516</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x41</th>   <td>-7777.9153</td> <td> 2790.923</td> <td>   -2.787</td> <td> 0.005</td> <td>-1.32e+04</td> <td>-2307.806</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x42</th>   <td> 5970.8491</td> <td> 2813.857</td> <td>    2.122</td> <td> 0.034</td> <td>  455.791</td> <td> 1.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x43</th>   <td> -723.4768</td> <td>  111.897</td> <td>   -6.466</td> <td> 0.000</td> <td> -942.792</td> <td> -504.162</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x44</th>   <td> -307.1707</td> <td>  127.886</td> <td>   -2.402</td> <td> 0.016</td> <td> -557.822</td> <td>  -56.519</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x45</th>   <td>  390.8045</td> <td>  127.582</td> <td>    3.063</td> <td> 0.002</td> <td>  140.748</td> <td>  640.861</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x46</th>   <td> -281.2068</td> <td>   43.488</td> <td>   -6.466</td> <td> 0.000</td> <td> -366.442</td> <td> -195.972</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x47</th>   <td> -146.5989</td> <td>   60.990</td> <td>   -2.404</td> <td> 0.016</td> <td> -266.137</td> <td>  -27.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x48</th>   <td>  177.5036</td> <td>   57.961</td> <td>    3.062</td> <td> 0.002</td> <td>   63.901</td> <td>  291.106</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x49</th>   <td>  -66.8453</td> <td>   10.307</td> <td>   -6.485</td> <td> 0.000</td> <td>  -87.047</td> <td>  -46.644</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x50</th>   <td>  -18.1554</td> <td>    7.579</td> <td>   -2.396</td> <td> 0.017</td> <td>  -33.009</td> <td>   -3.301</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x51</th>   <td>   29.1917</td> <td>    9.513</td> <td>    3.069</td> <td> 0.002</td> <td>   10.546</td> <td>   47.837</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x52</th>   <td> 1.089e+04</td> <td> 1682.325</td> <td>    6.470</td> <td> 0.000</td> <td> 7588.171</td> <td> 1.42e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x53</th>   <td> 4767.0069</td> <td> 1983.784</td> <td>    2.403</td> <td> 0.016</td> <td>  878.861</td> <td> 8655.153</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x54</th>   <td>-6032.0641</td> <td> 1967.336</td> <td>   -3.066</td> <td> 0.002</td> <td>-9887.973</td> <td>-2176.156</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x55</th>   <td>-1047.1636</td> <td>  446.356</td> <td>   -2.346</td> <td> 0.019</td> <td>-1922.006</td> <td> -172.321</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x56</th>   <td> 4172.1309</td> <td>  472.163</td> <td>    8.836</td> <td> 0.000</td> <td> 3246.708</td> <td> 5097.554</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x57</th>   <td> 5488.0225</td> <td>  451.457</td> <td>   12.156</td> <td> 0.000</td> <td> 4603.183</td> <td> 6372.862</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x58</th>   <td>-1391.0684</td> <td>  592.993</td> <td>   -2.346</td> <td> 0.019</td> <td>-2553.314</td> <td> -228.823</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x59</th>   <td> 5413.2949</td> <td>  612.562</td> <td>    8.837</td> <td> 0.000</td> <td> 4212.696</td> <td> 6613.894</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x60</th>   <td> 7362.9803</td> <td>  605.763</td> <td>   12.155</td> <td> 0.000</td> <td> 6175.706</td> <td> 8550.254</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x61</th>   <td> -295.9503</td> <td>  126.024</td> <td>   -2.348</td> <td> 0.019</td> <td> -542.952</td> <td>  -48.948</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x62</th>   <td> 1230.9956</td> <td>  139.293</td> <td>    8.837</td> <td> 0.000</td> <td>  957.987</td> <td> 1504.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x63</th>   <td> 1496.4727</td> <td>  123.104</td> <td>   12.156</td> <td> 0.000</td> <td> 1255.193</td> <td> 1737.752</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x64</th>   <td>  149.7632</td> <td> 1120.651</td> <td>    0.134</td> <td> 0.894</td> <td>-2046.673</td> <td> 2346.200</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x65</th>   <td>-6987.9479</td> <td> 1160.426</td> <td>   -6.022</td> <td> 0.000</td> <td>-9262.341</td> <td>-4713.555</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x66</th>   <td>-2480.6659</td> <td> 1176.793</td> <td>   -2.108</td> <td> 0.035</td> <td>-4787.138</td> <td> -174.194</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x67</th>   <td>-1358.3587</td> <td>  270.724</td> <td>   -5.017</td> <td> 0.000</td> <td>-1888.969</td> <td> -827.749</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x68</th>   <td>-1130.8464</td> <td>  297.777</td> <td>   -3.798</td> <td> 0.000</td> <td>-1714.479</td> <td> -547.214</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x69</th>   <td> -175.9178</td> <td>  276.943</td> <td>   -0.635</td> <td> 0.525</td> <td> -718.716</td> <td>  366.881</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x70</th>   <td>-1716.4980</td> <td>  342.085</td> <td>   -5.018</td> <td> 0.000</td> <td>-2386.972</td> <td>-1046.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x71</th>   <td>-1439.4959</td> <td>  378.988</td> <td>   -3.798</td> <td> 0.000</td> <td>-2182.299</td> <td> -696.693</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x72</th>   <td> -255.1200</td> <td>  402.933</td> <td>   -0.633</td> <td> 0.527</td> <td>-1044.855</td> <td>  534.615</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x73</th>   <td> -334.4336</td> <td>   66.649</td> <td>   -5.018</td> <td> 0.000</td> <td> -465.063</td> <td> -203.804</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x74</th>   <td> -266.0422</td> <td>   70.060</td> <td>   -3.797</td> <td> 0.000</td> <td> -403.358</td> <td> -128.727</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x75</th>   <td>  -49.3845</td> <td>   77.705</td> <td>   -0.636</td> <td> 0.525</td> <td> -201.684</td> <td>  102.914</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x76</th>   <td> 1581.0304</td> <td>  577.105</td> <td>    2.740</td> <td> 0.006</td> <td>  449.926</td> <td> 2712.135</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x77</th>   <td> 2411.2440</td> <td>  641.579</td> <td>    3.758</td> <td> 0.000</td> <td> 1153.772</td> <td> 3668.716</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x78</th>   <td> 4005.9552</td> <td>  647.526</td> <td>    6.187</td> <td> 0.000</td> <td> 2736.828</td> <td> 5275.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x79</th>   <td> 2147.4792</td> <td>  981.542</td> <td>    2.188</td> <td> 0.029</td> <td>  223.693</td> <td> 4071.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x80</th>   <td> -985.2441</td> <td> 1048.784</td> <td>   -0.939</td> <td> 0.348</td> <td>-3040.822</td> <td> 1070.334</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x81</th>   <td>-9433.2631</td> <td> 1026.170</td> <td>   -9.193</td> <td> 0.000</td> <td>-1.14e+04</td> <td>-7422.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x82</th>   <td>   -1.0235</td> <td>    0.522</td> <td>   -1.962</td> <td> 0.050</td> <td>   -2.046</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x83</th>   <td>    0.2417</td> <td>    0.523</td> <td>    0.462</td> <td> 0.644</td> <td>   -0.784</td> <td>    1.267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x84</th>   <td>    2.7309</td> <td>    0.321</td> <td>    8.506</td> <td> 0.000</td> <td>    2.102</td> <td>    3.360</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x85</th>   <td> -456.3870</td> <td>  208.630</td> <td>   -2.188</td> <td> 0.029</td> <td> -865.294</td> <td>  -47.480</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x86</th>   <td>  212.8359</td> <td>  226.472</td> <td>    0.940</td> <td> 0.347</td> <td> -231.041</td> <td>  656.713</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x87</th>   <td> 1858.1036</td> <td>  202.136</td> <td>    9.192</td> <td> 0.000</td> <td> 1461.925</td> <td> 2254.282</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x88</th>   <td>  -84.0357</td> <td>   38.332</td> <td>   -2.192</td> <td> 0.028</td> <td> -159.166</td> <td>   -8.906</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x89</th>   <td>   42.5288</td> <td>   45.480</td> <td>    0.935</td> <td> 0.350</td> <td>  -46.610</td> <td>  131.668</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x90</th>   <td>  338.4309</td> <td>   36.806</td> <td>    9.195</td> <td> 0.000</td> <td>  266.292</td> <td>  410.570</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x91</th>   <td>   -0.0884</td> <td>    0.030</td> <td>   -2.898</td> <td> 0.004</td> <td>   -0.148</td> <td>   -0.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x92</th>   <td>    0.2990</td> <td>    0.038</td> <td>    7.946</td> <td> 0.000</td> <td>    0.225</td> <td>    0.373</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x93</th>   <td>   -0.4126</td> <td>    0.039</td> <td>  -10.689</td> <td> 0.000</td> <td>   -0.488</td> <td>   -0.337</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x94</th>   <td>    0.3016</td> <td>    0.069</td> <td>    4.379</td> <td> 0.000</td> <td>    0.167</td> <td>    0.437</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x95</th>   <td>   -0.4965</td> <td>    0.078</td> <td>   -6.362</td> <td> 0.000</td> <td>   -0.649</td> <td>   -0.344</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x96</th>   <td>   -1.2321</td> <td>    0.101</td> <td>  -12.217</td> <td> 0.000</td> <td>   -1.430</td> <td>   -1.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x97</th>   <td>   -0.1364</td> <td>    0.035</td> <td>   -3.937</td> <td> 0.000</td> <td>   -0.204</td> <td>   -0.068</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x98</th>   <td>    0.1005</td> <td>    0.033</td> <td>    3.064</td> <td> 0.002</td> <td>    0.036</td> <td>    0.165</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x99</th>   <td>    0.3874</td> <td>    0.036</td> <td>   10.773</td> <td> 0.000</td> <td>    0.317</td> <td>    0.458</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x100</th>  <td>   -0.0644</td> <td>    0.026</td> <td>   -2.513</td> <td> 0.012</td> <td>   -0.115</td> <td>   -0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x101</th>  <td>   -0.0010</td> <td>    0.024</td> <td>   -0.044</td> <td> 0.965</td> <td>   -0.048</td> <td>    0.046</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x102</th>  <td>   -0.4220</td> <td>    0.028</td> <td>  -14.976</td> <td> 0.000</td> <td>   -0.477</td> <td>   -0.367</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x103</th>  <td>    0.0636</td> <td>    0.024</td> <td>    2.696</td> <td> 0.007</td> <td>    0.017</td> <td>    0.110</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x104</th>  <td>    0.1550</td> <td>    0.025</td> <td>    6.230</td> <td> 0.000</td> <td>    0.106</td> <td>    0.204</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x105</th>  <td>   -0.2471</td> <td>    0.033</td> <td>   -7.436</td> <td> 0.000</td> <td>   -0.312</td> <td>   -0.182</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x106</th>  <td>   -0.0748</td> <td>    0.040</td> <td>   -1.887</td> <td> 0.059</td> <td>   -0.153</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x107</th>  <td>    0.3443</td> <td>    0.041</td> <td>    8.465</td> <td> 0.000</td> <td>    0.265</td> <td>    0.424</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x108</th>  <td>   -0.3356</td> <td>    0.053</td> <td>   -6.363</td> <td> 0.000</td> <td>   -0.439</td> <td>   -0.232</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x109</th>  <td>   -0.1591</td> <td>    0.029</td> <td>   -5.504</td> <td> 0.000</td> <td>   -0.216</td> <td>   -0.102</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x110</th>  <td>   -0.1598</td> <td>    0.030</td> <td>   -5.239</td> <td> 0.000</td> <td>   -0.220</td> <td>   -0.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x111</th>  <td>   -0.3203</td> <td>    0.037</td> <td>   -8.716</td> <td> 0.000</td> <td>   -0.392</td> <td>   -0.248</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x112</th>  <td>    0.0078</td> <td>    0.040</td> <td>    0.193</td> <td> 0.847</td> <td>   -0.071</td> <td>    0.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x113</th>  <td>    0.0893</td> <td>    0.042</td> <td>    2.147</td> <td> 0.032</td> <td>    0.008</td> <td>    0.171</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x114</th>  <td>   -0.3242</td> <td>    0.050</td> <td>   -6.546</td> <td> 0.000</td> <td>   -0.421</td> <td>   -0.227</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x115</th>  <td>    0.0961</td> <td>    0.039</td> <td>    2.472</td> <td> 0.013</td> <td>    0.020</td> <td>    0.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x116</th>  <td>   -0.1160</td> <td>    0.042</td> <td>   -2.793</td> <td> 0.005</td> <td>   -0.197</td> <td>   -0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x117</th>  <td>   -0.3571</td> <td>    0.050</td> <td>   -7.188</td> <td> 0.000</td> <td>   -0.455</td> <td>   -0.260</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x118</th>  <td>    0.0720</td> <td>    0.029</td> <td>    2.497</td> <td> 0.013</td> <td>    0.016</td> <td>    0.129</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x119</th>  <td>    0.1469</td> <td>    0.035</td> <td>    4.157</td> <td> 0.000</td> <td>    0.078</td> <td>    0.216</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x120</th>  <td>   -0.1290</td> <td>    0.042</td> <td>   -3.085</td> <td> 0.002</td> <td>   -0.211</td> <td>   -0.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x121</th>  <td>   -0.2521</td> <td>    0.020</td> <td>  -12.860</td> <td> 0.000</td> <td>   -0.291</td> <td>   -0.214</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x122</th>  <td>    0.0029</td> <td>    0.038</td> <td>    0.076</td> <td> 0.939</td> <td>   -0.072</td> <td>    0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x123</th>  <td>   -0.0838</td> <td>    0.035</td> <td>   -2.385</td> <td> 0.017</td> <td>   -0.153</td> <td>   -0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x124</th>  <td>    0.0836</td> <td>    0.034</td> <td>    2.473</td> <td> 0.013</td> <td>    0.017</td> <td>    0.150</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x125</th>  <td>   -0.0305</td> <td>    0.017</td> <td>   -1.774</td> <td> 0.076</td> <td>   -0.064</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x126</th>  <td>    0.0104</td> <td>    0.024</td> <td>    0.440</td> <td> 0.660</td> <td>   -0.036</td> <td>    0.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x127</th>  <td>   -0.0042</td> <td>    0.020</td> <td>   -0.207</td> <td> 0.836</td> <td>   -0.044</td> <td>    0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x128</th>  <td>    0.1239</td> <td>    0.024</td> <td>    5.216</td> <td> 0.000</td> <td>    0.077</td> <td>    0.170</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x129</th>  <td>    0.0391</td> <td>    0.022</td> <td>    1.749</td> <td> 0.080</td> <td>   -0.005</td> <td>    0.083</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x130</th>  <td>   -0.0221</td> <td>    0.019</td> <td>   -1.148</td> <td> 0.251</td> <td>   -0.060</td> <td>    0.016</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x131</th>  <td>    0.1089</td> <td>    0.050</td> <td>    2.174</td> <td> 0.030</td> <td>    0.011</td> <td>    0.207</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x132</th>  <td>    0.0104</td> <td>    0.024</td> <td>    0.440</td> <td> 0.660</td> <td>   -0.036</td> <td>    0.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x133</th>  <td>    0.3184</td> <td>    0.051</td> <td>    6.283</td> <td> 0.000</td> <td>    0.219</td> <td>    0.418</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x134</th>  <td>    0.1239</td> <td>    0.024</td> <td>    5.216</td> <td> 0.000</td> <td>    0.077</td> <td>    0.170</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x135</th>  <td>   -0.3495</td> <td>    0.043</td> <td>   -8.177</td> <td> 0.000</td> <td>   -0.433</td> <td>   -0.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x136</th>  <td>   -0.0221</td> <td>    0.019</td> <td>   -1.148</td> <td> 0.251</td> <td>   -0.060</td> <td>    0.016</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x137</th>  <td>   -0.1056</td> <td>    0.055</td> <td>   -1.930</td> <td> 0.054</td> <td>   -0.213</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x138</th>  <td>   -0.0710</td> <td>    0.044</td> <td>   -1.608</td> <td> 0.108</td> <td>   -0.158</td> <td>    0.016</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x139</th>  <td>   -0.0462</td> <td>    0.079</td> <td>   -0.589</td> <td> 0.556</td> <td>   -0.200</td> <td>    0.108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x140</th>  <td>   -0.0310</td> <td>    0.007</td> <td>   -4.713</td> <td> 0.000</td> <td>   -0.044</td> <td>   -0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x141</th>  <td>   -0.0310</td> <td>    0.007</td> <td>   -4.713</td> <td> 0.000</td> <td>   -0.044</td> <td>   -0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x142</th>  <td>   -0.0310</td> <td>    0.007</td> <td>   -4.713</td> <td> 0.000</td> <td>   -0.044</td> <td>   -0.018</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:                37664\n",
       "Model:                            GLM   Df Residuals:                    37526\n",
       "Model Family:                Binomial   Df Model:                          137\n",
       "Link Function:                  logit   Scale:                          1.0000\n",
       "Method:                          IRLS   Log-Likelihood:                -15356.\n",
       "Date:                Mon, 13 May 2019   Deviance:                       30712.\n",
       "Time:                        20:12:46   Pearson chi2:                 8.91e+04\n",
       "No. Iterations:                   100   Covariance Type:             nonrobust\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         -1.6808      0.026    -64.105      0.000      -1.732      -1.629\n",
       "x1            -0.1817      0.066     -2.747      0.006      -0.311      -0.052\n",
       "x2             0.4209      0.079      5.354      0.000       0.267       0.575\n",
       "x3             1.2147      0.094     12.881      0.000       1.030       1.400\n",
       "x4            -6.1407      1.590     -3.863      0.000      -9.256      -3.025\n",
       "x5             1.9294      1.289      1.497      0.134      -0.597       4.456\n",
       "x6             1.8155      1.465      1.239      0.215      -1.056       4.688\n",
       "x7            -4.9840      1.641     -3.038      0.002      -8.200      -1.768\n",
       "x8             0.2462      1.289      0.191      0.849      -2.280       2.772\n",
       "x9             1.2653      1.505      0.841      0.400      -1.684       4.215\n",
       "x10            0.0653      0.023      2.879      0.004       0.021       0.110\n",
       "x11            0.0452      0.025      1.774      0.076      -0.005       0.095\n",
       "x12           -0.0086      0.016     -0.524      0.600      -0.041       0.024\n",
       "x13            1.3590      0.401      3.391      0.001       0.573       2.145\n",
       "x14           -0.0396      0.255     -0.155      0.877      -0.539       0.460\n",
       "x15           -0.2686      0.334     -0.803      0.422      -0.924       0.387\n",
       "x16         2127.0491    675.251      3.150      0.002     803.581    3450.517\n",
       "x17           62.3075    707.191      0.088      0.930   -1323.761    1448.376\n",
       "x18         2968.3592    685.743      4.329      0.000    1624.327    4312.392\n",
       "x19         2300.7460    730.718      3.149      0.002     868.564    3732.928\n",
       "x20           62.8421    704.481      0.089      0.929   -1317.915    1443.600\n",
       "x21         3026.1187    698.975      4.329      0.000    1656.153    4396.085\n",
       "x22          206.9519     65.653      3.152      0.002      78.275     335.629\n",
       "x23            5.2925     64.438      0.082      0.935    -121.004     131.589\n",
       "x24          254.5721     58.788      4.330      0.000     139.350     369.794\n",
       "x25           -0.1765      0.025     -7.065      0.000      -0.225      -0.128\n",
       "x26           -0.0751      0.032     -2.315      0.021      -0.139      -0.012\n",
       "x27            0.0817      0.027      2.996      0.003       0.028       0.135\n",
       "x28        -9540.7094   1361.105     -7.010      0.000   -1.22e+04   -6872.992\n",
       "x29        -2620.4693   1438.547     -1.822      0.069   -5439.970     199.031\n",
       "x30        -1547.4897   1429.622     -1.082      0.279   -4349.498    1254.518\n",
       "x31         -250.4705   1416.055     -0.177      0.860   -3025.887    2524.946\n",
       "x32         2298.8751   1525.117      1.507      0.132    -690.299    5288.049\n",
       "x33         -453.4524   1548.518     -0.293      0.770   -3488.492    2581.587\n",
       "x34         -254.3615   1430.844     -0.178      0.859   -3058.763    2550.040\n",
       "x35         2386.4856   1582.128      1.508      0.131    -714.428    5487.399\n",
       "x36         -466.6791   1594.913     -0.293      0.770   -3592.652    2659.294\n",
       "x37           -7.5623     42.401     -0.178      0.858     -90.668      75.543\n",
       "x38           72.0182     47.743      1.508      0.131     -21.556     165.592\n",
       "x39          -12.6715     43.066     -0.294      0.769     -97.080      71.737\n",
       "x40        -9226.9267   2490.562     -3.705      0.000   -1.41e+04   -4345.516\n",
       "x41        -7777.9153   2790.923     -2.787      0.005   -1.32e+04   -2307.806\n",
       "x42         5970.8491   2813.857      2.122      0.034     455.791    1.15e+04\n",
       "x43         -723.4768    111.897     -6.466      0.000    -942.792    -504.162\n",
       "x44         -307.1707    127.886     -2.402      0.016    -557.822     -56.519\n",
       "x45          390.8045    127.582      3.063      0.002     140.748     640.861\n",
       "x46         -281.2068     43.488     -6.466      0.000    -366.442    -195.972\n",
       "x47         -146.5989     60.990     -2.404      0.016    -266.137     -27.061\n",
       "x48          177.5036     57.961      3.062      0.002      63.901     291.106\n",
       "x49          -66.8453     10.307     -6.485      0.000     -87.047     -46.644\n",
       "x50          -18.1554      7.579     -2.396      0.017     -33.009      -3.301\n",
       "x51           29.1917      9.513      3.069      0.002      10.546      47.837\n",
       "x52         1.089e+04   1682.325      6.470      0.000    7588.171    1.42e+04\n",
       "x53         4767.0069   1983.784      2.403      0.016     878.861    8655.153\n",
       "x54        -6032.0641   1967.336     -3.066      0.002   -9887.973   -2176.156\n",
       "x55        -1047.1636    446.356     -2.346      0.019   -1922.006    -172.321\n",
       "x56         4172.1309    472.163      8.836      0.000    3246.708    5097.554\n",
       "x57         5488.0225    451.457     12.156      0.000    4603.183    6372.862\n",
       "x58        -1391.0684    592.993     -2.346      0.019   -2553.314    -228.823\n",
       "x59         5413.2949    612.562      8.837      0.000    4212.696    6613.894\n",
       "x60         7362.9803    605.763     12.155      0.000    6175.706    8550.254\n",
       "x61         -295.9503    126.024     -2.348      0.019    -542.952     -48.948\n",
       "x62         1230.9956    139.293      8.837      0.000     957.987    1504.004\n",
       "x63         1496.4727    123.104     12.156      0.000    1255.193    1737.752\n",
       "x64          149.7632   1120.651      0.134      0.894   -2046.673    2346.200\n",
       "x65        -6987.9479   1160.426     -6.022      0.000   -9262.341   -4713.555\n",
       "x66        -2480.6659   1176.793     -2.108      0.035   -4787.138    -174.194\n",
       "x67        -1358.3587    270.724     -5.017      0.000   -1888.969    -827.749\n",
       "x68        -1130.8464    297.777     -3.798      0.000   -1714.479    -547.214\n",
       "x69         -175.9178    276.943     -0.635      0.525    -718.716     366.881\n",
       "x70        -1716.4980    342.085     -5.018      0.000   -2386.972   -1046.024\n",
       "x71        -1439.4959    378.988     -3.798      0.000   -2182.299    -696.693\n",
       "x72         -255.1200    402.933     -0.633      0.527   -1044.855     534.615\n",
       "x73         -334.4336     66.649     -5.018      0.000    -465.063    -203.804\n",
       "x74         -266.0422     70.060     -3.797      0.000    -403.358    -128.727\n",
       "x75          -49.3845     77.705     -0.636      0.525    -201.684     102.914\n",
       "x76         1581.0304    577.105      2.740      0.006     449.926    2712.135\n",
       "x77         2411.2440    641.579      3.758      0.000    1153.772    3668.716\n",
       "x78         4005.9552    647.526      6.187      0.000    2736.828    5275.082\n",
       "x79         2147.4792    981.542      2.188      0.029     223.693    4071.266\n",
       "x80         -985.2441   1048.784     -0.939      0.348   -3040.822    1070.334\n",
       "x81        -9433.2631   1026.170     -9.193      0.000   -1.14e+04   -7422.007\n",
       "x82           -1.0235      0.522     -1.962      0.050      -2.046      -0.001\n",
       "x83            0.2417      0.523      0.462      0.644      -0.784       1.267\n",
       "x84            2.7309      0.321      8.506      0.000       2.102       3.360\n",
       "x85         -456.3870    208.630     -2.188      0.029    -865.294     -47.480\n",
       "x86          212.8359    226.472      0.940      0.347    -231.041     656.713\n",
       "x87         1858.1036    202.136      9.192      0.000    1461.925    2254.282\n",
       "x88          -84.0357     38.332     -2.192      0.028    -159.166      -8.906\n",
       "x89           42.5288     45.480      0.935      0.350     -46.610     131.668\n",
       "x90          338.4309     36.806      9.195      0.000     266.292     410.570\n",
       "x91           -0.0884      0.030     -2.898      0.004      -0.148      -0.029\n",
       "x92            0.2990      0.038      7.946      0.000       0.225       0.373\n",
       "x93           -0.4126      0.039    -10.689      0.000      -0.488      -0.337\n",
       "x94            0.3016      0.069      4.379      0.000       0.167       0.437\n",
       "x95           -0.4965      0.078     -6.362      0.000      -0.649      -0.344\n",
       "x96           -1.2321      0.101    -12.217      0.000      -1.430      -1.034\n",
       "x97           -0.1364      0.035     -3.937      0.000      -0.204      -0.068\n",
       "x98            0.1005      0.033      3.064      0.002       0.036       0.165\n",
       "x99            0.3874      0.036     10.773      0.000       0.317       0.458\n",
       "x100          -0.0644      0.026     -2.513      0.012      -0.115      -0.014\n",
       "x101          -0.0010      0.024     -0.044      0.965      -0.048       0.046\n",
       "x102          -0.4220      0.028    -14.976      0.000      -0.477      -0.367\n",
       "x103           0.0636      0.024      2.696      0.007       0.017       0.110\n",
       "x104           0.1550      0.025      6.230      0.000       0.106       0.204\n",
       "x105          -0.2471      0.033     -7.436      0.000      -0.312      -0.182\n",
       "x106          -0.0748      0.040     -1.887      0.059      -0.153       0.003\n",
       "x107           0.3443      0.041      8.465      0.000       0.265       0.424\n",
       "x108          -0.3356      0.053     -6.363      0.000      -0.439      -0.232\n",
       "x109          -0.1591      0.029     -5.504      0.000      -0.216      -0.102\n",
       "x110          -0.1598      0.030     -5.239      0.000      -0.220      -0.100\n",
       "x111          -0.3203      0.037     -8.716      0.000      -0.392      -0.248\n",
       "x112           0.0078      0.040      0.193      0.847      -0.071       0.087\n",
       "x113           0.0893      0.042      2.147      0.032       0.008       0.171\n",
       "x114          -0.3242      0.050     -6.546      0.000      -0.421      -0.227\n",
       "x115           0.0961      0.039      2.472      0.013       0.020       0.172\n",
       "x116          -0.1160      0.042     -2.793      0.005      -0.197      -0.035\n",
       "x117          -0.3571      0.050     -7.188      0.000      -0.455      -0.260\n",
       "x118           0.0720      0.029      2.497      0.013       0.016       0.129\n",
       "x119           0.1469      0.035      4.157      0.000       0.078       0.216\n",
       "x120          -0.1290      0.042     -3.085      0.002      -0.211      -0.047\n",
       "x121          -0.2521      0.020    -12.860      0.000      -0.291      -0.214\n",
       "x122           0.0029      0.038      0.076      0.939      -0.072       0.078\n",
       "x123          -0.0838      0.035     -2.385      0.017      -0.153      -0.015\n",
       "x124           0.0836      0.034      2.473      0.013       0.017       0.150\n",
       "x125          -0.0305      0.017     -1.774      0.076      -0.064       0.003\n",
       "x126           0.0104      0.024      0.440      0.660      -0.036       0.057\n",
       "x127          -0.0042      0.020     -0.207      0.836      -0.044       0.036\n",
       "x128           0.1239      0.024      5.216      0.000       0.077       0.170\n",
       "x129           0.0391      0.022      1.749      0.080      -0.005       0.083\n",
       "x130          -0.0221      0.019     -1.148      0.251      -0.060       0.016\n",
       "x131           0.1089      0.050      2.174      0.030       0.011       0.207\n",
       "x132           0.0104      0.024      0.440      0.660      -0.036       0.057\n",
       "x133           0.3184      0.051      6.283      0.000       0.219       0.418\n",
       "x134           0.1239      0.024      5.216      0.000       0.077       0.170\n",
       "x135          -0.3495      0.043     -8.177      0.000      -0.433      -0.266\n",
       "x136          -0.0221      0.019     -1.148      0.251      -0.060       0.016\n",
       "x137          -0.1056      0.055     -1.930      0.054      -0.213       0.002\n",
       "x138          -0.0710      0.044     -1.608      0.108      -0.158       0.016\n",
       "x139          -0.0462      0.079     -0.589      0.556      -0.200       0.108\n",
       "x140          -0.0310      0.007     -4.713      0.000      -0.044      -0.018\n",
       "x141          -0.0310      0.007     -4.713      0.000      -0.044      -0.018\n",
       "x142          -0.0310      0.007     -4.713      0.000      -0.044      -0.018\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic regression model\n",
    "logm1 = sm.GLM(y_train_balance,(sm.add_constant(X_train_balance)), family = sm.families.Binomial())\n",
    "logm1.fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Feature Selection Using RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\i503427\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "rfe = RFE(logreg, 30)             # running RFE with 13 variables as output\n",
    "rfe = rfe.fit(X_train_balance, y_train_balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True,  True,  True, False, False, False,  True,  True,\n",
       "       False, False, False, False,  True, False,  True, False, False,\n",
       "       False,  True, False,  True,  True, False, False, False, False,\n",
       "       False, False,  True,  True, False, False, False,  True, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False,  True, False, False, False,  True,  True, False, False,\n",
       "       False,  True,  True, False,  True,  True, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False,  True, False, False, False, False, False, False, False,\n",
       "       False,  True,  True, False,  True,  True, False, False, False,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False, False,  True, False, False,  True, False, False,  True,\n",
       "       False, False, False,  True, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfe.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('arpu_6', False, 45),\n",
       " ('arpu_7', True, 1),\n",
       " ('arpu_8', True, 1),\n",
       " ('onnet_mou_6', True, 1),\n",
       " ('onnet_mou_7', False, 5),\n",
       " ('onnet_mou_8', False, 97),\n",
       " ('offnet_mou_6', False, 109),\n",
       " ('offnet_mou_7', True, 1),\n",
       " ('offnet_mou_8', True, 1),\n",
       " ('roam_ic_mou_6', False, 35),\n",
       " ('roam_ic_mou_7', False, 77),\n",
       " ('roam_ic_mou_8', False, 108),\n",
       " ('roam_og_mou_6', False, 59),\n",
       " ('roam_og_mou_7', True, 1),\n",
       " ('roam_og_mou_8', False, 20),\n",
       " ('loc_og_t2t_mou_6', True, 1),\n",
       " ('loc_og_t2t_mou_7', False, 54),\n",
       " ('loc_og_t2t_mou_8', False, 4),\n",
       " ('loc_og_t2m_mou_6', False, 24),\n",
       " ('loc_og_t2m_mou_7', True, 1),\n",
       " ('loc_og_t2m_mou_8', False, 95),\n",
       " ('loc_og_t2f_mou_6', True, 1),\n",
       " ('loc_og_t2f_mou_7', True, 1),\n",
       " ('loc_og_t2f_mou_8', False, 91),\n",
       " ('loc_og_t2c_mou_6', False, 23),\n",
       " ('loc_og_t2c_mou_7', False, 55),\n",
       " ('loc_og_t2c_mou_8', False, 57),\n",
       " ('loc_og_mou_6', False, 47),\n",
       " ('loc_og_mou_7', False, 64),\n",
       " ('loc_og_mou_8', True, 1),\n",
       " ('std_og_t2t_mou_6', True, 1),\n",
       " ('std_og_t2t_mou_7', False, 6),\n",
       " ('std_og_t2t_mou_8', False, 3),\n",
       " ('std_og_t2m_mou_6', False, 63),\n",
       " ('std_og_t2m_mou_7', True, 1),\n",
       " ('std_og_t2m_mou_8', False, 62),\n",
       " ('std_og_t2f_mou_6', False, 101),\n",
       " ('std_og_t2f_mou_7', False, 96),\n",
       " ('std_og_t2f_mou_8', False, 53),\n",
       " ('std_og_mou_6', False, 69),\n",
       " ('std_og_mou_7', False, 52),\n",
       " ('std_og_mou_8', False, 93),\n",
       " ('isd_og_mou_6', False, 44),\n",
       " ('isd_og_mou_7', False, 8),\n",
       " ('isd_og_mou_8', False, 9),\n",
       " ('spl_og_mou_6', False, 111),\n",
       " ('spl_og_mou_7', False, 112),\n",
       " ('spl_og_mou_8', False, 43),\n",
       " ('og_others_6', False, 37),\n",
       " ('og_others_7', False, 65),\n",
       " ('og_others_8', False, 100),\n",
       " ('total_og_mou_6', False, 46),\n",
       " ('total_og_mou_7', False, 42),\n",
       " ('total_og_mou_8', False, 56),\n",
       " ('loc_ic_t2t_mou_6', False, 90),\n",
       " ('loc_ic_t2t_mou_7', True, 1),\n",
       " ('loc_ic_t2t_mou_8', False, 86),\n",
       " ('loc_ic_t2m_mou_6', False, 36),\n",
       " ('loc_ic_t2m_mou_7', False, 27),\n",
       " ('loc_ic_t2m_mou_8', True, 1),\n",
       " ('loc_ic_t2f_mou_6', True, 1),\n",
       " ('loc_ic_t2f_mou_7', False, 28),\n",
       " ('loc_ic_t2f_mou_8', False, 74),\n",
       " ('loc_ic_mou_6', False, 88),\n",
       " ('loc_ic_mou_7', True, 1),\n",
       " ('loc_ic_mou_8', True, 1),\n",
       " ('std_ic_t2t_mou_6', False, 73),\n",
       " ('std_ic_t2t_mou_7', True, 1),\n",
       " ('std_ic_t2t_mou_8', True, 1),\n",
       " ('std_ic_t2m_mou_6', False, 75),\n",
       " ('std_ic_t2m_mou_7', False, 84),\n",
       " ('std_ic_t2m_mou_8', False, 71),\n",
       " ('std_ic_t2f_mou_6', False, 103),\n",
       " ('std_ic_t2f_mou_7', False, 66),\n",
       " ('std_ic_t2f_mou_8', False, 15),\n",
       " ('std_ic_mou_6', False, 72),\n",
       " ('std_ic_mou_7', False, 85),\n",
       " ('std_ic_mou_8', False, 2),\n",
       " ('total_ic_mou_6', False, 11),\n",
       " ('total_ic_mou_7', False, 83),\n",
       " ('total_ic_mou_8', False, 48),\n",
       " ('spl_ic_mou_6', False, 51),\n",
       " ('spl_ic_mou_7', True, 1),\n",
       " ('spl_ic_mou_8', False, 10),\n",
       " ('isd_ic_mou_6', False, 89),\n",
       " ('isd_ic_mou_7', False, 50),\n",
       " ('isd_ic_mou_8', False, 92),\n",
       " ('ic_others_6', False, 16),\n",
       " ('ic_others_7', False, 33),\n",
       " ('ic_others_8', False, 49),\n",
       " ('total_rech_num_6', False, 58),\n",
       " ('total_rech_num_7', True, 1),\n",
       " ('total_rech_num_8', True, 1),\n",
       " ('total_rech_amt_6', False, 25),\n",
       " ('total_rech_amt_7', True, 1),\n",
       " ('total_rech_amt_8', True, 1),\n",
       " ('max_rech_amt_6', False, 38),\n",
       " ('max_rech_amt_7', False, 40),\n",
       " ('max_rech_amt_8', False, 7),\n",
       " ('last_day_rch_amt_6', False, 76),\n",
       " ('last_day_rch_amt_7', False, 104),\n",
       " ('last_day_rch_amt_8', True, 1),\n",
       " ('vol_2g_mb_6', False, 60),\n",
       " ('vol_2g_mb_7', False, 22),\n",
       " ('vol_2g_mb_8', False, 21),\n",
       " ('vol_3g_mb_6', False, 80),\n",
       " ('vol_3g_mb_7', False, 13),\n",
       " ('vol_3g_mb_8', False, 12),\n",
       " ('monthly_2g_6', False, 26),\n",
       " ('monthly_2g_7', False, 17),\n",
       " ('monthly_2g_8', True, 1),\n",
       " ('sachet_2g_6', False, 81),\n",
       " ('sachet_2g_7', False, 29),\n",
       " ('sachet_2g_8', True, 1),\n",
       " ('monthly_3g_6', False, 68),\n",
       " ('monthly_3g_7', False, 61),\n",
       " ('monthly_3g_8', True, 1),\n",
       " ('sachet_3g_6', False, 70),\n",
       " ('sachet_3g_7', False, 31),\n",
       " ('sachet_3g_8', False, 32),\n",
       " ('aon', True, 1),\n",
       " ('aug_vbc_3g', False, 102),\n",
       " ('jul_vbc_3g', False, 78),\n",
       " ('jun_vbc_3g', False, 79),\n",
       " ('night_pck_6_1.0', False, 99),\n",
       " ('night_pck_6_2.0', False, 107),\n",
       " ('night_pck_7_1.0', False, 106),\n",
       " ('night_pck_7_2.0', False, 19),\n",
       " ('night_pck_8_1.0', False, 98),\n",
       " ('night_pck_8_2.0', False, 105),\n",
       " ('fb_6_1.0', False, 39),\n",
       " ('fb_6_2.0', False, 113),\n",
       " ('fb_7_1.0', False, 18),\n",
       " ('fb_7_2.0', False, 34),\n",
       " ('fb_8_1.0', False, 14),\n",
       " ('fb_8_2.0', False, 110),\n",
       " ('total_data_amt_6', False, 67),\n",
       " ('total_data_amt_7', False, 30),\n",
       " ('total_data_amt_8', False, 87),\n",
       " ('days_forrech_before_month_end_6', False, 82),\n",
       " ('days_forrech_before_month_end_7', False, 94),\n",
       " ('days_forrech_before_month_end_8', False, 41)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(X.columns, rfe.support_, rfe.ranking_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Choosing only the rfe selected feature as the set of columns\n",
    "\n",
    "col = X.columns[rfe.support_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assessing the model with StatsModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rfe = pd.DataFrame(data=X_train_balance).iloc[:, rfe.support_]\n",
    "y_train_rfe = y_train_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>y</td>        <th>  No. Observations:  </th>  <td> 37664</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>GLM</td>       <th>  Df Residuals:      </th>  <td> 37633</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>       <td>Binomial</td>     <th>  Df Model:          </th>  <td>    30</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>        <td>logit</td>      <th>  Scale:             </th> <td>  1.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -16493.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>           <td>Mon, 13 May 2019</td> <th>  Deviance:          </th> <td>  32987.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>               <td>20:19:14</td>     <th>  Pearson chi2:      </th> <td>1.90e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>         <td>6</td>        <th>  Covariance Type:   </th> <td>nonrobust</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>   -1.5931</td> <td>    0.025</td> <td>  -64.511</td> <td> 0.000</td> <td>   -1.642</td> <td>   -1.545</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1</th>     <td>    0.5745</td> <td>    0.062</td> <td>    9.200</td> <td> 0.000</td> <td>    0.452</td> <td>    0.697</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2</th>     <td>    0.6124</td> <td>    0.075</td> <td>    8.203</td> <td> 0.000</td> <td>    0.466</td> <td>    0.759</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3</th>     <td>   -0.4190</td> <td>    0.136</td> <td>   -3.075</td> <td> 0.002</td> <td>   -0.686</td> <td>   -0.152</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7</th>     <td>   -1.0308</td> <td>    0.216</td> <td>   -4.766</td> <td> 0.000</td> <td>   -1.455</td> <td>   -0.607</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8</th>     <td>   -0.5154</td> <td>    0.038</td> <td>  -13.425</td> <td> 0.000</td> <td>   -0.591</td> <td>   -0.440</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13</th>    <td>    0.4806</td> <td>    0.033</td> <td>   14.397</td> <td> 0.000</td> <td>    0.415</td> <td>    0.546</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15</th>    <td>    0.4763</td> <td>    0.073</td> <td>    6.552</td> <td> 0.000</td> <td>    0.334</td> <td>    0.619</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>19</th>    <td>    0.6565</td> <td>    0.109</td> <td>    6.042</td> <td> 0.000</td> <td>    0.444</td> <td>    0.869</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>21</th>    <td>    0.3277</td> <td>    0.037</td> <td>    8.758</td> <td> 0.000</td> <td>    0.254</td> <td>    0.401</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22</th>    <td>   -0.3254</td> <td>    0.047</td> <td>   -6.979</td> <td> 0.000</td> <td>   -0.417</td> <td>   -0.234</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>29</th>    <td>   -0.7263</td> <td>    0.056</td> <td>  -13.052</td> <td> 0.000</td> <td>   -0.835</td> <td>   -0.617</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>30</th>    <td>    0.4756</td> <td>    0.119</td> <td>    4.006</td> <td> 0.000</td> <td>    0.243</td> <td>    0.708</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>34</th>    <td>    1.1683</td> <td>    0.186</td> <td>    6.268</td> <td> 0.000</td> <td>    0.803</td> <td>    1.534</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>55</th>    <td>   -0.4651</td> <td>    0.058</td> <td>   -8.021</td> <td> 0.000</td> <td>   -0.579</td> <td>   -0.351</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>59</th>    <td>   -0.8486</td> <td>    0.112</td> <td>   -7.592</td> <td> 0.000</td> <td>   -1.068</td> <td>   -0.630</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>60</th>    <td>   -0.2969</td> <td>    0.035</td> <td>   -8.551</td> <td> 0.000</td> <td>   -0.365</td> <td>   -0.229</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>64</th>    <td>    1.2147</td> <td>    0.062</td> <td>   19.456</td> <td> 0.000</td> <td>    1.092</td> <td>    1.337</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>65</th>    <td>   -1.1837</td> <td>    0.116</td> <td>  -10.205</td> <td> 0.000</td> <td>   -1.411</td> <td>   -0.956</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>67</th>    <td>    0.3915</td> <td>    0.036</td> <td>   11.005</td> <td> 0.000</td> <td>    0.322</td> <td>    0.461</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>68</th>    <td>   -0.9403</td> <td>    0.063</td> <td>  -14.817</td> <td> 0.000</td> <td>   -1.065</td> <td>   -0.816</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>82</th>    <td>   -0.3070</td> <td>    0.036</td> <td>   -8.579</td> <td> 0.000</td> <td>   -0.377</td> <td>   -0.237</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>91</th>    <td>    0.3166</td> <td>    0.027</td> <td>   11.885</td> <td> 0.000</td> <td>    0.264</td> <td>    0.369</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>92</th>    <td>   -0.6961</td> <td>    0.034</td> <td>  -20.460</td> <td> 0.000</td> <td>   -0.763</td> <td>   -0.629</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>94</th>    <td>   -0.3263</td> <td>    0.056</td> <td>   -5.799</td> <td> 0.000</td> <td>   -0.437</td> <td>   -0.216</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>95</th>    <td>   -0.6420</td> <td>    0.075</td> <td>   -8.513</td> <td> 0.000</td> <td>   -0.790</td> <td>   -0.494</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>101</th>   <td>   -0.3596</td> <td>    0.025</td> <td>  -14.511</td> <td> 0.000</td> <td>   -0.408</td> <td>   -0.311</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>110</th>   <td>   -0.6809</td> <td>    0.026</td> <td>  -26.491</td> <td> 0.000</td> <td>   -0.731</td> <td>   -0.631</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>113</th>   <td>   -0.4685</td> <td>    0.023</td> <td>  -20.383</td> <td> 0.000</td> <td>   -0.514</td> <td>   -0.423</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>116</th>   <td>   -0.5855</td> <td>    0.027</td> <td>  -21.386</td> <td> 0.000</td> <td>   -0.639</td> <td>   -0.532</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>120</th>   <td>   -0.3104</td> <td>    0.018</td> <td>  -17.237</td> <td> 0.000</td> <td>   -0.346</td> <td>   -0.275</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:                37664\n",
       "Model:                            GLM   Df Residuals:                    37633\n",
       "Model Family:                Binomial   Df Model:                           30\n",
       "Link Function:                  logit   Scale:                          1.0000\n",
       "Method:                          IRLS   Log-Likelihood:                -16493.\n",
       "Date:                Mon, 13 May 2019   Deviance:                       32987.\n",
       "Time:                        20:19:14   Pearson chi2:                 1.90e+05\n",
       "No. Iterations:                     6   Covariance Type:             nonrobust\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         -1.5931      0.025    -64.511      0.000      -1.642      -1.545\n",
       "1              0.5745      0.062      9.200      0.000       0.452       0.697\n",
       "2              0.6124      0.075      8.203      0.000       0.466       0.759\n",
       "3             -0.4190      0.136     -3.075      0.002      -0.686      -0.152\n",
       "7             -1.0308      0.216     -4.766      0.000      -1.455      -0.607\n",
       "8             -0.5154      0.038    -13.425      0.000      -0.591      -0.440\n",
       "13             0.4806      0.033     14.397      0.000       0.415       0.546\n",
       "15             0.4763      0.073      6.552      0.000       0.334       0.619\n",
       "19             0.6565      0.109      6.042      0.000       0.444       0.869\n",
       "21             0.3277      0.037      8.758      0.000       0.254       0.401\n",
       "22            -0.3254      0.047     -6.979      0.000      -0.417      -0.234\n",
       "29            -0.7263      0.056    -13.052      0.000      -0.835      -0.617\n",
       "30             0.4756      0.119      4.006      0.000       0.243       0.708\n",
       "34             1.1683      0.186      6.268      0.000       0.803       1.534\n",
       "55            -0.4651      0.058     -8.021      0.000      -0.579      -0.351\n",
       "59            -0.8486      0.112     -7.592      0.000      -1.068      -0.630\n",
       "60            -0.2969      0.035     -8.551      0.000      -0.365      -0.229\n",
       "64             1.2147      0.062     19.456      0.000       1.092       1.337\n",
       "65            -1.1837      0.116    -10.205      0.000      -1.411      -0.956\n",
       "67             0.3915      0.036     11.005      0.000       0.322       0.461\n",
       "68            -0.9403      0.063    -14.817      0.000      -1.065      -0.816\n",
       "82            -0.3070      0.036     -8.579      0.000      -0.377      -0.237\n",
       "91             0.3166      0.027     11.885      0.000       0.264       0.369\n",
       "92            -0.6961      0.034    -20.460      0.000      -0.763      -0.629\n",
       "94            -0.3263      0.056     -5.799      0.000      -0.437      -0.216\n",
       "95            -0.6420      0.075     -8.513      0.000      -0.790      -0.494\n",
       "101           -0.3596      0.025    -14.511      0.000      -0.408      -0.311\n",
       "110           -0.6809      0.026    -26.491      0.000      -0.731      -0.631\n",
       "113           -0.4685      0.023    -20.383      0.000      -0.514      -0.423\n",
       "116           -0.5855      0.027    -21.386      0.000      -0.639      -0.532\n",
       "120           -0.3104      0.018    -17.237      0.000      -0.346      -0.275\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sm = sm.add_constant(X_train_rfe)\n",
    "logm2 = sm.GLM(y_train_rfe,X_train_sm, family = sm.families.Binomial())\n",
    "res = logm2.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.143403\n",
       "1    0.148379\n",
       "2    0.749978\n",
       "3    0.782227\n",
       "4    0.160934\n",
       "5    0.037214\n",
       "6    0.739097\n",
       "7    0.140984\n",
       "8    0.031555\n",
       "9    0.043683\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the predicted values on the train set\n",
    "y_train_pred = res.predict(X_train_sm)\n",
    "y_train_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14340279, 0.14837914, 0.74997751, 0.78222694, 0.16093387,\n",
       "       0.03721397, 0.73909734, 0.14098393, 0.0315555 , 0.04368279])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred = y_train_pred.values.reshape(-1)\n",
    "y_train_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a dataframe with the actual churn flag and the predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>churn</th>\n",
       "      <th>Churn_Prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.143403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.148379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.749978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.782227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.160934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   churn  Churn_Prob\n",
       "0      0    0.143403\n",
       "1      0    0.148379\n",
       "2      0    0.749978\n",
       "3      0    0.782227\n",
       "4      0    0.160934"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred_final = pd.DataFrame({'churn':y_train_rfe, 'Churn_Prob':y_train_pred})\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>churn</th>\n",
       "      <th>Churn_Prob</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.143403</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.148379</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.749978</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.782227</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.160934</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   churn  Churn_Prob  predicted\n",
       "0      0    0.143403          0\n",
       "1      0    0.148379          0\n",
       "2      0    0.749978          1\n",
       "3      0    0.782227          1\n",
       "4      0    0.160934          0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "# Let's see the head\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14731  4101]\n",
      " [ 3004 15828]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix \n",
    "confusion = metrics.confusion_matrix(y_train_pred_final.churn, y_train_pred_final.predicted )\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted     not_churn    churn\n",
    "# Actual\n",
    "# not_churn        14927    3903\n",
    "# churn            3107       15723  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.811358326253186\n"
     ]
    }
   ],
   "source": [
    "# Let's check the overall accuracy.\n",
    "print(metrics.accuracy_score(y_train_pred_final.churn, y_train_pred_final.predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>13</th>\n",
       "      <th>15</th>\n",
       "      <th>19</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>...</th>\n",
       "      <th>82</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>101</th>\n",
       "      <th>110</th>\n",
       "      <th>113</th>\n",
       "      <th>116</th>\n",
       "      <th>120</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.651262</td>\n",
       "      <td>-0.623588</td>\n",
       "      <td>-0.578246</td>\n",
       "      <td>-0.775681</td>\n",
       "      <td>-0.721164</td>\n",
       "      <td>-0.210483</td>\n",
       "      <td>-0.375524</td>\n",
       "      <td>-0.691663</td>\n",
       "      <td>-0.290483</td>\n",
       "      <td>-0.297826</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108913</td>\n",
       "      <td>-0.946395</td>\n",
       "      <td>-0.894135</td>\n",
       "      <td>-0.635677</td>\n",
       "      <td>-0.577404</td>\n",
       "      <td>-0.656413</td>\n",
       "      <td>-0.385289</td>\n",
       "      <td>-0.425133</td>\n",
       "      <td>1.286999</td>\n",
       "      <td>1.421897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.016426</td>\n",
       "      <td>-0.830254</td>\n",
       "      <td>-0.578246</td>\n",
       "      <td>-0.775681</td>\n",
       "      <td>-0.721164</td>\n",
       "      <td>-0.210483</td>\n",
       "      <td>-0.375524</td>\n",
       "      <td>-0.691663</td>\n",
       "      <td>-0.290483</td>\n",
       "      <td>-0.297826</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108913</td>\n",
       "      <td>-0.524020</td>\n",
       "      <td>-0.142532</td>\n",
       "      <td>-0.994602</td>\n",
       "      <td>-0.783631</td>\n",
       "      <td>-0.482236</td>\n",
       "      <td>-0.385289</td>\n",
       "      <td>2.307925</td>\n",
       "      <td>-0.336358</td>\n",
       "      <td>-0.909653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.864972</td>\n",
       "      <td>-0.986036</td>\n",
       "      <td>-0.434823</td>\n",
       "      <td>-0.443625</td>\n",
       "      <td>-0.658340</td>\n",
       "      <td>-0.210483</td>\n",
       "      <td>-0.091884</td>\n",
       "      <td>-0.021638</td>\n",
       "      <td>-0.290483</td>\n",
       "      <td>-0.297826</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108913</td>\n",
       "      <td>-0.946395</td>\n",
       "      <td>-1.001507</td>\n",
       "      <td>-1.016927</td>\n",
       "      <td>-0.940757</td>\n",
       "      <td>-0.447400</td>\n",
       "      <td>-0.385289</td>\n",
       "      <td>-0.425133</td>\n",
       "      <td>-0.336358</td>\n",
       "      <td>1.730972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.081439</td>\n",
       "      <td>0.123816</td>\n",
       "      <td>0.060800</td>\n",
       "      <td>-0.477767</td>\n",
       "      <td>-0.226600</td>\n",
       "      <td>-0.210483</td>\n",
       "      <td>0.222712</td>\n",
       "      <td>-0.340328</td>\n",
       "      <td>-0.290483</td>\n",
       "      <td>-0.278592</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108913</td>\n",
       "      <td>2.115824</td>\n",
       "      <td>1.682789</td>\n",
       "      <td>0.013478</td>\n",
       "      <td>0.139483</td>\n",
       "      <td>-0.447400</td>\n",
       "      <td>-0.385289</td>\n",
       "      <td>-0.425133</td>\n",
       "      <td>-0.336358</td>\n",
       "      <td>-0.439816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.319728</td>\n",
       "      <td>-0.289937</td>\n",
       "      <td>4.533992</td>\n",
       "      <td>0.420569</td>\n",
       "      <td>0.546050</td>\n",
       "      <td>-0.210483</td>\n",
       "      <td>9.734719</td>\n",
       "      <td>1.160909</td>\n",
       "      <td>-0.276736</td>\n",
       "      <td>-0.297826</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108913</td>\n",
       "      <td>-0.840801</td>\n",
       "      <td>-0.786763</td>\n",
       "      <td>-0.312817</td>\n",
       "      <td>-0.351535</td>\n",
       "      <td>0.249308</td>\n",
       "      <td>-0.385289</td>\n",
       "      <td>-0.425133</td>\n",
       "      <td>-0.336358</td>\n",
       "      <td>1.701932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        1         2         3         7         8         13        15   \\\n",
       "0 -0.651262 -0.623588 -0.578246 -0.775681 -0.721164 -0.210483 -0.375524   \n",
       "1 -1.016426 -0.830254 -0.578246 -0.775681 -0.721164 -0.210483 -0.375524   \n",
       "2 -0.864972 -0.986036 -0.434823 -0.443625 -0.658340 -0.210483 -0.091884   \n",
       "3  0.081439  0.123816  0.060800 -0.477767 -0.226600 -0.210483  0.222712   \n",
       "4 -0.319728 -0.289937  4.533992  0.420569  0.546050 -0.210483  9.734719   \n",
       "\n",
       "        19        21        22     ...          82        91        92   \\\n",
       "0 -0.691663 -0.290483 -0.297826    ...    -0.108913 -0.946395 -0.894135   \n",
       "1 -0.691663 -0.290483 -0.297826    ...    -0.108913 -0.524020 -0.142532   \n",
       "2 -0.021638 -0.290483 -0.297826    ...    -0.108913 -0.946395 -1.001507   \n",
       "3 -0.340328 -0.290483 -0.278592    ...    -0.108913  2.115824  1.682789   \n",
       "4  1.160909 -0.276736 -0.297826    ...    -0.108913 -0.840801 -0.786763   \n",
       "\n",
       "        94        95        101       110       113       116       120  \n",
       "0 -0.635677 -0.577404 -0.656413 -0.385289 -0.425133  1.286999  1.421897  \n",
       "1 -0.994602 -0.783631 -0.482236 -0.385289  2.307925 -0.336358 -0.909653  \n",
       "2 -1.016927 -0.940757 -0.447400 -0.385289 -0.425133 -0.336358  1.730972  \n",
       "3  0.013478  0.139483 -0.447400 -0.385289 -0.425133 -0.336358 -0.439816  \n",
       "4 -0.312817 -0.351535  0.249308 -0.385289 -0.425133 -0.336358  1.701932  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = pd.DataFrame(data=X_test).iloc[:, rfe.support_]\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sm = sm.add_constant(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = res.predict(X_test_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.009867\n",
       "1    0.377061\n",
       "2    0.628647\n",
       "3    0.115134\n",
       "4    0.059572\n",
       "5    0.049363\n",
       "6    0.686649\n",
       "7    0.023962\n",
       "8    0.254687\n",
       "9    0.106289\n",
       "dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting y_pred to a dataframe which is an array\n",
    "y_pred_1 = pd.DataFrame(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting y_test to dataframe\n",
    "y_test_df = pd.DataFrame(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing index for both dataframes to append them side by side \n",
    "y_pred_1.reset_index(drop=True, inplace=True)\n",
    "y_test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending y_test_df and y_pred_1\n",
    "y_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>churn</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.009867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.377061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.628647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.115134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.059572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   churn         0\n",
       "0      0  0.009867\n",
       "1      0  0.377061\n",
       "2      0  0.628647\n",
       "3      0  0.115134\n",
       "4      0  0.059572"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming the column \n",
    "y_pred_final= y_pred_final.rename(columns={ 0 : 'Churn_Prob'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_final['final_predicted'] = y_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.55 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>churn</th>\n",
       "      <th>Churn_Prob</th>\n",
       "      <th>final_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.009867</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.377061</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.628647</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.115134</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.059572</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   churn  Churn_Prob  final_predicted\n",
       "0      0    0.009867                0\n",
       "1      0    0.377061                0\n",
       "2      0    0.628647                1\n",
       "3      0    0.115134                0\n",
       "4      0    0.059572                0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8180542563143124"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the overall accuracy.\n",
    "metrics.accuracy_score(y_pred_final.churn, y_pred_final.final_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6619, 1455],\n",
       "       [ 101,  377]], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion2 = metrics.confusion_matrix(y_pred_final.churn, y_pred_final.final_predicted )\n",
    "confusion2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion2[1,1] # true positive \n",
    "TN = confusion2[0,0] # true negatives\n",
    "FP = confusion2[0,1] # false positives\n",
    "FN = confusion2[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7887029288702929"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see the sensitivity of our logistic regression model\n",
    "TP / float(TP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8197919246965568"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us calculate specificity\n",
    "TN / float(TN+FP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
